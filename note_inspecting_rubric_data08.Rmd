---
title: "Note to Inspect Rubric Data 8"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook:
    toc: yes
    toc_float:
      collapsed: yes
    code_folding: hide
  html_document:
    toc: yes
    toc_float:
      collapsed: no
    df_print: paged
    code_folding: hide
    fig_height: 7
    fig_width: 7
---

```{r, echo=FALSE, include=FALSE}
#knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
options(dplyr.summarise.inform=FALSE) 
library(readxl)
```

Objective
================================================================================
PLO assessments are mostly collected and validated. Add some analyses useful for
Standards 4 & 6. Note 7 is still active. Be mindful of updates in data sets.

Start with additional visuals from the past reports (delta) and ones requested
by Beth (disaggregates).


Set up
================================================================================
Import Data
--------------------------------------------------------------------------------
Codes are externalized for reuse. As a reference, data frames are as follows:

- `mdf` or main data frame contains the rubric outcome data merged with the course/assessment information gathered from the survey. Data are cleaned up.
- `non` or non-rubric data frame is separated from the same data source as the
rubric information of `mdf`. It contains the instructor's assessments of
outcomes at course level. Data are cleaned up.
- `past` data frame contains the assessment outcome data from the past.
- `rowmap` data frame summarizes the mapping between PLOs and rubric rows.

```{r}
source('read.R')
source('munge.R')
```
Those scripts have been updated to use the *edited* version of the survey result
data. See Note 5 for details. Given that change, the Notebooks 1-5 are
completely outdated.

New Issues
--------------------------------------------------------------------------------
New things in this note:

- [x] Drop BUS 499 GW1 Spring 2020 Annotated Bibliography from data, while
adding the observed into non-rubric
- [x] Drop ECO 421 001 from data, while adding the observed into non-rubric
- [x] Drop HRM 400 001 from data, while adding the observed into non-rubric

```{r}
mdf.original <- mdf
mdf <- mdf %>%
  filter(!( (Section == "BUS 499 GW1" & Semester == "Spring 2020" &
               Assessment == "Annotated Bibliography") |
             Section == "ECO 421 001" | Section == "HRM 400 001"))
```

Then scraped data will be re-introduced in the following.


PLO Measures for Standard 4
================================================================================
Overview
--------------------------------------------------------------------------------
For Standard 4, we only need a rather small subset of the assessments available.
Produce the summary of outcomes in the context of historical changes.

- ACBSP requires 3-5 data points, so make sure to have sufficient numbers.
- ACBSP requires graphs with sample sizes.
- Nuventive requires numbers. Produce data tables along with the graphs.

Once Standard 4 is done, we can discuss weakness/shortcomings and
strength/improvements in our program, then look at other courses - including the
ones at an earlier stage of each program - to find *opportunities for
improvement* and *close the loop*. That is, we can get to **Standard 6**
analysis.


Summarize Outcomes
--------------------------------------------------------------------------------
For this academic year, we have assessments both with and without rubrics. In
the following, I summarize the rubric-based assessments - observed at individual
level - at `Section` level, then combine them with the non-rubric-based
assessments - observed at `Section` level. Then, the summarized data will be
aggregated to `Course` level.

The individual data has `r nrow(mdf)` rows, while there are `r
n_distinct(mdf$UserId)` distinctive students, `r n_distinct(mdf$RubricId)`
rubrics with `r n_distinct(mdf$Name)` rows for `r n_distinct(mdf$Course)`
courses. Implementations of those courses are distinguished by `r
n_distinct(mdf$Section)` `Sections` over three `Semester`.

The data pulled from D2L contains Rubric ID, but not associated with courses or
instructors. Such additional information is provided through the survey
collected. They are merged in the script above and issues like inconsistent
names and typos have been handled, resulting in `mdf` data frame. Meanwhile, the
survey also contains the information about non-rubric assessments. They are
cleaned and separated into `non` data frame.

The individual data are necessary for analyzing the grade distributions and
possible correlations in various grades. However, Standard 4 only requires the
aggregates.  Therefore, I will summarize the outcomes at the aggregation level
of course and rubric row.

First, the individual-level data are aggregated to
`Course`-`Section`-`Semester`-`Assessment`-`Name`(Rubric Row) level ("Section &
Rubric Row" level). At this stage, each implementation is distinguishable.
Instructor level analysis can be done. In the aggregation, proportions of
students who met the goal are calculated.

Second, the Section & Rubric Row level data above is combined with the rubric
data *scraped from the D2L directly*. There are some missing in the retrieved
D2L data. As a temporary measure, student outcomes are collected from D2L course
pages manually. The results are summarized at the rubric row levels; hence
compatible with the aggregated data mentioned above.

The non-rubric assessments mentioned before are merged at this stage. However,
unlike the scraped data, those non-rubric assessments do not have rows. (All the
rows are labeled "No Rubric".) If both rubric and non-rubric assessments exist,
they need to be distinguished (Use `Rubric` flag).

Third, the data are aggregated to the course level, erasing the distinction
between sections. This level is used for student outcome assessment. The
resulting data is distinguished at `Course`-`Assessment`-`Name`(Rubric Row).

```{r}
tbl1 <- mdf %>% filter(str_detect(Name, "Major|Minor", negate = TRUE)) %>%
  group_by(Course, Section, Semester, Assessment, Name, PLO) %>%
  summarise(Met.UND = mean(Met.UND.bin), Met.GRD = mean(Met.GRD.bin),
            n = n_distinct(UserId),
            Rubric = TRUE)
miss <- read_excel("data/D2L Missing Data Retrievals.xlsx", "Scraped")
miss <- miss %>%
  select(-c(Unsatisfactory:Advanced)) %>% mutate(Met.UND = Met, Met.GRD = Met)
tbl2 <- bind_rows(tbl1, miss)
tbl3 <- tbl2 %>%
  bind_rows(non %>% transmute(Course, Section, Semester, Assessment,
                              Name = "No Rubric", PLO,
                              Met.UND = np/n, Met.GRD = Met.UND, n,
                              Rubric = FALSE))
tbl4 <- tbl3 %>% 
  group_by(Course, Assessment, Name, PLO, Rubric) %>%
  summarise(Met.UND = weighted.mean(Met.UND, n, na.rm = TRUE),
            Met.GRD = weighted.mean(Met.GRD, n, na.rm = TRUE),
            n = sum(n, na.rm = TRUE), n.sec = n_distinct(Section)
            ) %>% 
  ungroup()
```

Prepare
--------------------------------------------------------------------------------
All the `Course`-`Assessment`-`Name`(Rubric Row) level observations are
summarized in `tbl4`. In order to get Program Learning Objective (PLO) level
aggregation, they need to be aggregated following the specifications summarized
in mapping keys.

```{r}
get_outcome <- function(plotag, rrow, program, plo, 
                        grad = FALSE, ori = NA, type = NA,
                        target = NA, unit = NA, goal = NA){
  # Extract assessment by PLO Tags and Rubric Row (pattern matching)
  if(is.na(rrow)){
    return(
      tibble(Rubric = NA, n = NA,
             Program = program, PG = NA, PLO = plo,
             Orientation = ori, Type = type,
             AY = 2019, Target = FALSE, Unit = "Prevalence", Goal = NA,
             Met = NA))}
  x <- tbl4 %>% filter(PLO == plotag, str_detect(Name, rrow))
  if(nrow(x) == 0){
    warning(paste("Criteria result in empty dataset for", plotag, ".\n"))
    return(NA)
    }
  out <- x %>% group_by(Rubric) %>% 
      summarize(Met.GRD = mean(Met.GRD), Met.UND = mean(Met.UND), n = min(n)) %>%
      mutate(Program = program, PG = NA, PLO = plo,
             Orientation = ori, Type = type,
             AY = 2019, Target = target, Unit = "Prevalence", Goal = goal)
  if(grad){
    out %>% mutate(Met = Met.GRD) %>% select(-starts_with("Met."))
  }
  else{
    out %>% mutate(Met = Met.UND) %>% select(-starts_with("Met."))
  }
}
#get_outcome(plotag = "ACC 1", rrow = "NA", program = "Accounting", plo = 1)
#get_outcome(plotag = "FIN 1", rrow = "No Rubric", program = "Finance", plo = 1)
```
```{r}
keys.all <- read_csv("data/mapping_keys.csv", na = "NA")
#knitr::kable(keys.all)
```
```{r}
get_outcomes <- function(program, grad = FALSE){
  # Wrapper to return a combined results for each program
  keys <- keys.all %>% filter(Program == program)
  lres <- list()
  plos <- keys$PLO
  plotags <- keys$PLO.Tags
  rrows <- keys$Rubric.Rows
  orientations <- keys$Orientation
  types <- keys$Type
  goals <- keys$Goals
  for(i in 1:length(plotags)){
    lres[[i]] <- get_outcome(plotags[i], rrows[i], program, plos[i],
                             grad, orientations[i], types[i],
                             NA, NA, goals[i])
  }
  lres
}

bind_outcomes <- function(lres){
  bind_rows(lres) %>% ungroup() %>%
    transmute(Program, `Goal Number` = PG,
           `Learning Objective Number` = PLO,
           `Assessment Type Orientation` = Orientation,
           `Assessment Type` = Type, `Academic Year` = AY,
           `Sample Size` = n, Outcome = Met,
           `Outcome Unit` = Unit, `Outcome Goal` = Goal, Rubric = Rubric)
}

getbind_outcomes <- function(program, grad = FALSE){
  lres <- get_outcomes(program, grad)
  bind_outcomes(lres)
}
#get_outcomes("Accounting")
#get_outcomes("Business Core")
#get_outcomes("Finance")
#getbind_outcomes("Finance")
```

Now that all the functions are prepared, extract rows from `tbl4` and merge them
together. The end result is a collection of AY2019 outcomes. Furthermore, merge the 2019 results into the exisiting results (`past`).
```{r}
out2019 <- getbind_outcomes("Accounting") %>%
  bind_rows(getbind_outcomes("Business Core")) %>% 
  bind_rows(getbind_outcomes("Economics")) %>% 
  bind_rows(getbind_outcomes("Finance")) %>% 
  bind_rows(getbind_outcomes("Human Resources Management")) %>% 
  bind_rows(getbind_outcomes("Management")) %>% 
#  bind_rows(getbind_outcomes("Marketing")) %>% 
  bind_rows(getbind_outcomes("MBA")) %>% 
  bind_rows(past)
```

Export the data.
```{r}
write_csv(out2019, "export/outcome2019.csv")
```

Visualize
--------------------------------------------------------------------------------
Now that all the outcomes are in one place. I can simply filter relevant results and visualize them. First, define a wrapper function to standardize the format:
```{r}
draw_graph <- function(x, program, note = ""){
  # Wrapper function to draw trends in outcomes.
  title <- paste0(program,
                  ", Prevalence of Achievments Over Academic Years, by PLO")
  caption <- "Grey number represents sample size."
  caption <- paste0(caption, "\n", note)
  ymin <- min(x$Outcome - 0.1, x$`Outcome Goal` - 0.1, na.rm = TRUE)
  p <- ggplot(x, aes(`Academic Year`, Outcome)) +
    geom_line(na.rm = TRUE) +
    geom_point(aes(color = Outcome > `Outcome Goal`), na.rm = TRUE) +
    scale_colour_manual(values = setNames(c('black','red'),c(T, F))) +
    geom_text(aes(`Academic Year`, ymin + 0.05, label = round(`Sample Size`, 0)),
              alpha = 0.4, na.rm = TRUE) +
    geom_hline(aes(yintercept = `Outcome Goal`),
               alpha = 0.4, color = "red", na.rm = TRUE) +
    scale_y_continuous(labels=scales::percent, limits = c(ymin, 1))
  p + facet_wrap(vars(`Learning Objective Number`)) +
    labs(title = title, caption = caption) +
    theme(axis.title = element_blank(),
          text = element_text(family = "serif"),
          panel.grid.minor.x = element_blank(),
          legend.position = "none")
}
```

Then, retrieve the relevant parts of the table from `out2019` by specifying program and assessment types. Except for MBA, programs only have one type (summative/formative) per PLO. So, no need to specify them.
```{r}
lres <- list()
for(prog in c("Accounting", "Business Core", "Economics", "Finance", 
              "Human Resources Management", "Management")){
  lres[[prog]] <- out2019 %>%
    filter(Program == prog, `Assessment Type Orientation` == "Internal")
}
lres[["MBA"]] <- out2019 %>%
  filter(Program == "MBA",
         `Assessment Type Orientation` == "Internal",
         `Assessment Type` == "Formative")
```

Make graphs.
```{r}
lplt <- list()
for(prog in c("Accounting", "Business Core", "Economics", "Finance",
              "Human Resources Management", "Management", "MBA")){
  p <- draw_graph(lres[[prog]], prog)
  ggsave(p, file = paste0("fig/", prog, ".png"))
  lplt[[prog]] <- p
}

```

Print out the graphs for inspection. Usually no need to include in notes.
```{r, eval = FALSE}
sapply(lplt, print)
```


Some additional notes:
- ACC 318 001 (PLO 3) was taught by Rogan Howard in Spring 2020. See if exam data can be retrieved?
- ACC 441 GW1 (PLO 4) was taught by Christopher Lyons in Fall 2019. At least, Josh has the data from his section.
- MKT 741 (PLO 5) hasn't been offered for 2 years (?), so expected to have missing data.



Additional Visuals
================================================================================

Delta
--------------------------------------------------------------------------------
How far are we to achieve goals? Comparison with bar graphs is good, but taking
the difference from the goal (*delta*) is better. Following the past reports by
Ed. Make such graphs.

```{r}
dfres <- bind_rows(lres)
x <- dfres %>% filter(`Academic Year` == 2019) %>% mutate(Diff = Outcome - `Outcome Goal`)
p <- ggplot(x, aes(as.factor(`Learning Objective Number`), Diff)) +
  geom_col(na.rm = TRUE) + scale_y_continuous(labels=scales::percent) +
    geom_hline(yintercept = c(0.2, -0.2), alpha = 0.4, color = "red") +
    geom_hline(yintercept = c(0), alpha = 0.8, color = "black")
p <- p + facet_wrap(vars(Program), scales = "free_x") + 
    labs(title = "Deviations from Goals, AY2019, by Program and PLO") +
    theme(axis.title = element_blank(),
          text = element_text(family = "serif"),
          panel.grid.minor.x = element_blank(),
          legend.position = "none")
p
```

```{r}
ggsave(p, file = paste0("fig/", "deltas2019", ".png"))
```

Business Core Disaggregate
--------------------------------------------------------------------------------
Filter out attributes:
```{r}
attr <- mdf %>% filter(str_detect(Name, "Major|Minor")) %>% 
  select(UserId, Course, Section, Semester, Assessment, Name, LevelAchieved.Original)
```

Confirm `UserId` is unique.
```{r}
attr %>% filter(Course == "BUS 499")
```

```{r}
attr %>% filter(UserId == 129918)
```

Then, I can just collect one observation per student.
```{r}
program <- attr %>% select(UserId, Name, LevelAchieved.Original)
#x <- levels(factor(program$Name))
#str_extract(x, "(Major)|(Minor)")
program <- program %>%
  mutate(Name = str_extract(Name, "(Major)|(Minor)")) %>% distinct()
```

Not uniquely valued because selections are different...
```{r}
table(program$UserId)
program %>% filter(UserId == 270389)
```

Alleviate this issue by selecting course.
```{r}
program <- attr %>% select(UserId, Name, LevelAchieved.Original, Course, Assessment)
mm.bus499 <- program %>% filter(Course == "BUS 499")
mm.bus499 <- mm.bus499 %>%
  mutate(Name = str_extract(Name, "(Major)|(Minor)")) %>% distinct()
table(mm.bus499$UserId)
mm.bus499 %>% filter(UserId == 295879)
```

What if limit to Majors?
```{r}
mm.bus499 <- mm.bus499 %>% filter(Name == "Major")
mm.bus499 %>% filter(UserId == 441464)
```

Really? I need to fix those names. Also, note that it looks worse than actually is due to the empty levels.
```{r}
mm.bus499 <- droplevels(mm.bus499)
levels(mm.bus499$LevelAchieved.Original)
```

```{r}
mm.bus499 <- mm.bus499 %>%
  mutate(Major = str_to_title(LevelAchieved.Original))
table(mm.bus499$Major)
```

```{r}
#x <- levels(factor(mm.bus499$Major))
#x
#str_extract(x, "General Busines{1,3}")
#str_extract(x, "Integrated Global Business.*")
pttrn.major <- c("General Busines{1,3}" = "General Business",
                 "Integrated Global Business.*" = "Integrated Global Business")
mm.bus499 <- mm.bus499 %>% mutate(Major = str_replace_all(Major, pttrn.major))
table(mm.bus499$Major)
table(mm.bus499$UserId)
mm.bus499 %>% filter(UserId == 512078)
```

Still duplicates exist.

Alternatively, I can just 'match' by rubric ID, giving up conisistency...


Next
================================================================================