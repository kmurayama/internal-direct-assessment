---
title: "Report for ACBSP Standard 4"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
    df_print: paged
    code_folding: hide
    fig_height: 7
    fig_width: 7
---

```{r, echo=FALSE, include=FALSE}
#knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
options(dplyr.summarise.inform=FALSE) 
library(readxl)
```

Objective
================================================================================
In this document (and script), I produce the set of information needed for
Standard 4 in ACBSP reporting. For later reference, I document the procedures.


Set up
================================================================================
Import Data
--------------------------------------------------------------------------------
Codes are externalized for reuse. As a reference, data frames are as follows:

- `mdf` or main data frame contains the rubric outcome data merged with the
  course/assessment information gathered from the survey. Data are cleaned up.
- `non` or non-rubric data frame is separated from the same data source as the
  rubric information of `mdf`. It contains the instructor's assessments of
  outcomes at course level. Data are cleaned up.
- `past` data frame contains the assessment outcome data from the past.
- `rowmap` data frame summarizes the mapping between PLOs and rubric rows.

```{r}
source('read.R')
source('munge.R')
```

For the development of these codes, refer to Note 1-6.

Missing Observations Replaced
--------------------------------------------------------------------------------
There is an issue that has not been solved yet. Namely, some of student assessment outcomes are missing from the data in an apparently random way. Severe cases are identified and the relevant data are collected manually.

```{r}
mdf.original <- mdf
mdf <- mdf %>%
  filter(!( (Section == "BUS 499 GW1" & Semester == "Spring 2020" &
               Assessment == "Annotated Bibliography") |
             Section == "ECO 421 001" | Section == "HRM 400 001"))
```

Summarize Outcomes
--------------------------------------------------------------------------------
For this academic year (AY2019), we have assessments both with and without rubrics. In
the following, I summarize the rubric-based assessments - observed at individual
level - at `Section` level, then combine them with the non-rubric-based
assessments - observed at `Section` level. Then, the summarized data will be
aggregated to `Course` level.

The individual data has `r nrow(mdf)` rows, while there are `r
n_distinct(mdf$UserId)` distinctive students, `r n_distinct(mdf$RubricId)`
rubrics with `r n_distinct(mdf$Name)` rows for `r n_distinct(mdf$Course)`
courses. Implementations of those courses are distinguished by `r
n_distinct(mdf$Section)` `Sections` over three `Semester`.

The data pulled from D2L contains Rubric ID, but not associated with courses or
instructors. Such additional information is provided through the survey
collected. They are merged in the script above and issues like inconsistent
names and typos have been handled, resulting in `mdf` data frame. Meanwhile, the
survey also contains the information about non-rubric assessments. They are
cleaned and separated into `non` data frame.

The individual data are necessary for analyzing the grade distributions and
possible correlations in various grades. However, Standard 4 only requires the
aggregates.  Therefore, I will summarize the outcomes at the aggregation level
of course and rubric row.

First, the individual-level data are aggregated to
`Course`-`Section`-`Semester`-`Assessment`-`Name`(Rubric Row) level ("Section &
Rubric Row" level). At this stage, each implementation is distinguishable.
Instructor level analysis can be done. In the aggregation, proportions of
students who met the goal are calculated.

Second, the Section & Rubric Row level data above is combined with the rubric
data *scraped from the D2L directly*. There are some missing in the retrieved
D2L data. As a temporary measure, student outcomes are collected from D2L course
pages manually. The results are summarized at the rubric row levels; hence
compatible with the aggregated data mentioned above.

The non-rubric assessments mentioned before are merged at this stage. However,
unlike the scraped data, those non-rubric assessments do not have rows. (All the
rows are labeled "No Rubric".) If both rubric and non-rubric assessments exist,
they need to be distinguished (Use `Rubric` flag).

Third, the data are aggregated to the course level, erasing the distinction
between sections. This level is used for student outcome assessment. The
resulting data is distinguished at `Course`-`Assessment`-`Name`(Rubric Row).

```{r}
tbl1 <- mdf %>% filter(str_detect(Name, "Major|Minor", negate = TRUE)) %>%
  group_by(Course, Section, Semester, Assessment, Name, PLO) %>%
  summarise(Met.UND = mean(Met.UND.bin), Met.GRD = mean(Met.GRD.bin),
            n = n_distinct(UserId),
            Rubric = TRUE)
miss <- read_excel("data/D2L Missing Data Retrievals.xlsx", "Scraped")
miss <- miss %>%
  select(-c(Unsatisfactory:Advanced)) %>% mutate(Met.UND = Met, Met.GRD = Met)
tbl2 <- bind_rows(tbl1, miss)
tbl3 <- tbl2 %>%
  bind_rows(non %>% transmute(Course, Section, Semester, Assessment,
                              Name = "No Rubric", PLO,
                              Met.UND = np/n, Met.GRD = Met.UND, n,
                              Rubric = FALSE))
tbl4 <- tbl3 %>% 
  group_by(Course, Assessment, Name, PLO, Rubric) %>%
  summarise(Met.UND = weighted.mean(Met.UND, n, na.rm = TRUE),
            Met.GRD = weighted.mean(Met.GRD, n, na.rm = TRUE),
            n = sum(n, na.rm = TRUE), n.sec = n_distinct(Section)
            ) %>% 
  ungroup()
```

Extract Relevant Information
--------------------------------------------------------------------------------
All the `Course`-`Assessment`-`Name`(Rubric Row) level observations are
summarized in `tbl4`. In order to get Program Learning Objective (PLO) level
aggregation, they need to be aggregated following the specifications summarized
in mapping keys.

```{r}
get_outcome <- function(plotag, rrow, program, plo, 
                        grad = FALSE, ori = NA, type = NA,
                        target = NA, unit = NA, goal = NA){
  # Extract assessment by PLO Tags and Rubric Row (pattern matching)
  if(is.na(rrow)){
    return(
      tibble(Rubric = NA, n = NA,
             Program = program, PG = NA, PLO = plo,
             Orientation = ori, Type = type,
             AY = 2019, Target = FALSE, Unit = "Prevalence", Goal = NA,
             Met = NA))}
  x <- tbl4 %>% filter(PLO == plotag, str_detect(Name, rrow))
  if(nrow(x) == 0){
    warning(paste("Criteria result in empty dataset for", plotag, ".\n"))
    return(NA)
    }
  out <- x %>% group_by(Rubric) %>% 
      summarize(Met.GRD = mean(Met.GRD), Met.UND = mean(Met.UND), n = min(n)) %>%
      mutate(Program = program, PG = NA, PLO = plo,
             Orientation = ori, Type = type,
             AY = 2019, Target = target, Unit = "Prevalence", Goal = goal)
  if(grad){
    out %>% mutate(Met = Met.GRD) %>% select(-starts_with("Met."))
  }
  else{
    out %>% mutate(Met = Met.UND) %>% select(-starts_with("Met."))
  }
}
```
```{r, message=FALSE}
keys.all <- read_csv("data/mapping_keys.csv", na = "NA")
```
```{r}
get_outcomes <- function(program, grad = FALSE){
  # Wrapper to return a combined results for each program
  keys <- keys.all %>% filter(Program == program)
  lres <- list()
  plos <- keys$PLO
  plotags <- keys$PLO.Tags
  rrows <- keys$Rubric.Rows
  orientations <- keys$Orientation
  types <- keys$Type
  goals <- keys$Goals
  for(i in 1:length(plotags)){
    lres[[i]] <- get_outcome(plotags[i], rrows[i], program, plos[i],
                             grad, orientations[i], types[i],
                             NA, NA, goals[i])
  }
  lres
}

bind_outcomes <- function(lres){
  bind_rows(lres) %>% ungroup() %>%
    transmute(Program, `Goal Number` = PG,
           `Learning Objective Number` = PLO,
           `Assessment Type Orientation` = Orientation,
           `Assessment Type` = Type, `Academic Year` = AY,
           `Sample Size` = n, Outcome = Met,
           `Outcome Unit` = Unit, `Outcome Goal` = Goal, Rubric = Rubric)
}

getbind_outcomes <- function(program, grad = FALSE){
  lres <- get_outcomes(program, grad)
  bind_outcomes(lres)
}
```

Now that all the functions are prepared, extract rows from `tbl4` and merge them
together. The end result is a collection of AY2019 outcomes. Furthermore, merge the 2019 results into the existing results (`past`).
```{r}
out2019 <- getbind_outcomes("Accounting") %>%
  bind_rows(getbind_outcomes("Business Core")) %>% 
  bind_rows(getbind_outcomes("Economics")) %>% 
  bind_rows(getbind_outcomes("Finance")) %>% 
  bind_rows(getbind_outcomes("Human Resources Management")) %>% 
  bind_rows(getbind_outcomes("Management")) %>% 
#  bind_rows(getbind_outcomes("Marketing")) %>% 
  bind_rows(getbind_outcomes("MBA")) %>% 
  bind_rows(past)
```

Export the data.
```{r}
write_csv(out2019, "export/outcome2019.csv")
```

Visualize
--------------------------------------------------------------------------------
Now that all the outcomes are in one place. I can simply filter relevant results and visualize them. First, define a wrapper function to standardize the format:
```{r}
draw_graph <- function(x, program, note = ""){
  # Wrapper function to draw trends in outcomes.
  title <- paste0(program,
                  ", Prevalence of Achievments Over Academic Years, by PLO")
  caption <- "Grey number represents sample size."
  caption <- paste0(caption, "\n", note)
  ymin <- min(x$Outcome - 0.1, x$`Outcome Goal` - 0.1, na.rm = TRUE)
  p <- ggplot(x, aes(`Academic Year`, Outcome)) +
    geom_line(na.rm = TRUE) +
    geom_point(aes(color = Outcome > `Outcome Goal`), na.rm = TRUE) +
    scale_colour_manual(values = setNames(c('black','red'),c(T, F))) +
    geom_text(aes(`Academic Year`, ymin + 0.05, label = round(`Sample Size`, 0)),
              alpha = 0.4, na.rm = TRUE) +
    geom_hline(aes(yintercept = `Outcome Goal`),
               alpha = 0.4, color = "red", na.rm = TRUE) +
    scale_y_continuous(labels=scales::percent, limits = c(ymin, 1))
  p + facet_wrap(vars(`Learning Objective Number`)) +
    labs(title = title, caption = caption) +
    theme(axis.title = element_blank(),
          text = element_text(family = "serif"),
          panel.grid.minor.x = element_blank(),
          legend.position = "none")
}
```

Then, retrieve the relevant parts of the table from `out2019` by specifying program and assessment types. Except for MBA, programs only have one type (summative/formative) per PLO. So, no need to specify them.
```{r}
lres <- list()
for(prog in c("Accounting", "Business Core", "Economics", "Finance", 
              "Human Resources Management", "Management")){
  lres[[prog]] <- out2019 %>%
    filter(Program == prog, `Assessment Type Orientation` == "Internal")
}
lres[["MBA"]] <- out2019 %>%
  filter(Program == "MBA",
         `Assessment Type Orientation` == "Internal",
         `Assessment Type` == "Formative")
```

Make graphs.
```{r}
lplt <- list()
for(prog in c("Accounting", "Business Core", "Economics", "Finance",
              "Human Resources Management", "Management", "MBA")){
  p <- draw_graph(lres[[prog]], prog)
  ggsave(p, file = paste0("fig/", prog, ".png"))
  lplt[[prog]] <- p
}

```

Print out the graphs for inspection. Usually no need to include in notes.
```{r, eval = FALSE}
sapply(lplt, print)
```
